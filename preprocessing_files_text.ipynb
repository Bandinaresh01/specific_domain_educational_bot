{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8m1QKgRaV1cD"
      },
      "outputs": [],
      "source": [
        "%pip install -U langchain-community pypdf\n",
        "%pip install -U langchain-community pypdf\n",
        "!pip install unstructured\n",
        "!pip install langchain\n",
        "!pip install -q --upgrade \\\n",
        "  google-generativeai \\\n",
        "  gradio \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  faiss-cpu \\\n",
        "  pypdf \\\n",
        "  sentence-transformers \\\n",
        "  unstructured[pptx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ad33d3",
        "outputId": "013be749-5d43-4e16-a822-ea02e6a49d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/extracted_data\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"/content/data_base_pdf.zip\"\n",
        "extract_path = \"/content/extracted_data\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Files extracted to: {extract_path}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file at {zip_file_path} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_CYDLYyWsta"
      },
      "outputs": [],
      "source": [
        "#load pdf documents\n",
        "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
        "data_path=\"/content/extracted_data\"\n",
        "#function to load pdfs from the subjects floader\n",
        "def load_pdf_files(data_path):\n",
        "    loader=DirectoryLoader(data_path,glob=\"*pdf\",loader_cls=PyPDFLoader)\n",
        "    documents=loader.load()\n",
        "    return documents\n",
        "#loading those files from floder\n",
        "def load_subject(data_path):\n",
        "    subject_data={}\n",
        "    for subject in os.listdir(data_path):\n",
        "      subject_path=os.path.join(data_path,subject)\n",
        "      if os.path.isdir(subject_path):\n",
        "        subject_documents=load_pdf_files(subject_path)\n",
        "        subject_data[subject]=subject_documents\n",
        "    return subject_data\n",
        "\n",
        "subject_data=load_subject(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f42225aa",
        "outputId": "1058faf9-4e9c-4681-ca4c-1eba9cd15832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìö Loading subject: big_data\n",
            "  üñºÔ∏è Loaded PPT: Chapter 5.pptx (1 slides)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:incorrect startxref pointer(3)\n",
            "WARNING:pypdf._reader:parsing for Object Streams\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  üñºÔ∏è Loaded PPT: Chapter 4.pptx (1 slides)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 66 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 110 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  üìÑ Loaded PDF: UNIT-1 BDA R21 A7902 ABP.pdf (17 pages)\n",
            "  üìÑ Loaded PDF: Unit-2 Hadoop BDA R21 A7902 ABP.pdf (18 pages)\n",
            "  üñºÔ∏è Loaded PPT: Chapter 2.pptx (1 slides)\n",
            "  üñºÔ∏è Loaded PPT: Chapter 1.pptx (1 slides)\n",
            "  üñºÔ∏è Loaded PPT: Chapter 3.pptx (1 slides)\n",
            "\n",
            "üìö Loading subject: pr\n",
            "  üìÑ Loaded PDF: PYTHON PROGRAMMING NOTES.pdf (142 pages)\n",
            "  üìÑ Loaded PDF: UNIT-3 (PR).pdf (14 pages)\n",
            "  üìÑ Loaded PDF: Pattern Classification by Richard O. Duda, David G. Stork, Peter E.Hart .pdf (738 pages)\n",
            "  üìÑ Loaded PDF: 273_PATTERN RECOGNITION.pdf (139 pages)\n",
            "  üìÑ Loaded PDF: UNIT-4.pdf (22 pages)\n",
            "\n",
            "üìö Loading subject: ccv\n",
            "  üìÑ Loaded PDF: CCV UNIT-I.pdf (26 pages)\n",
            "  üìÑ Loaded PDF: UNIT-V Cloud Computing and its Applications.pdf (11 pages)\n",
            "  üìÑ Loaded PDF: CCV UNIT-II.pdf (13 pages)\n",
            "  üìÑ Loaded PDF: UNIT-IV Secure distributed data storage in Cloud.pdf (26 pages)\n",
            "  üìÑ Loaded PDF: UNIT-III Virtualization.pdf (32 pages)\n",
            "\n",
            "‚úÖ All subjects loaded!\n",
            "\n",
            "big_data: 40 documents\n",
            "pr: 1055 documents\n",
            "ccv: 108 documents\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, UnstructuredPowerPointLoader\n",
        "\n",
        "# ‚úÖ Folder that contains subject folders\n",
        "data_base_path = \"/content/extracted_data/data_base_pdf\"\n",
        "\n",
        "# ‚úÖ This will hold all subject documents\n",
        "subject_documents = {}\n",
        "\n",
        "# ‚úÖ Loop through each subject folder\n",
        "for subject_name in os.listdir(data_base_path):\n",
        "    subject_path = os.path.join(data_base_path, subject_name)\n",
        "\n",
        "    # Make sure it's a folder\n",
        "    if os.path.isdir(subject_path):\n",
        "        print(f\"\\nüìö Loading subject: {subject_name}\")\n",
        "        docs = []\n",
        "\n",
        "        # ‚úÖ Loop through each file inside the subject folder\n",
        "        for file_name in os.listdir(subject_path):\n",
        "            file_path = os.path.join(subject_path, file_name)\n",
        "\n",
        "            try:\n",
        "                # ‚úÖ If it's a PDF file\n",
        "                if file_name.endswith(\".pdf\"):\n",
        "                    loader = PyPDFLoader(file_path)\n",
        "                    file_docs = loader.load()\n",
        "                    docs.extend(file_docs)\n",
        "                    print(f\"  üìÑ Loaded PDF: {file_name} ({len(file_docs)} pages)\")\n",
        "\n",
        "                # ‚úÖ If it's a PowerPoint file\n",
        "                elif file_name.endswith(\".pptx\") or file_name.endswith(\".ppt\"):\n",
        "                    loader = UnstructuredPowerPointLoader(file_path)\n",
        "                    ppt_docs = loader.load()\n",
        "                    docs.extend(ppt_docs)\n",
        "                    print(f\"  üñºÔ∏è Loaded PPT: {file_name} ({len(ppt_docs)} slides)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Failed to load {file_name}: {str(e)}\")\n",
        "\n",
        "        # ‚úÖ Store all loaded documents for this subject\n",
        "        subject_documents[subject_name] = docs\n",
        "\n",
        "# ‚úÖ Final check\n",
        "print(\"\\n‚úÖ All subjects loaded!\\n\")\n",
        "for subject, docs in subject_documents.items():\n",
        "    print(f\"{subject}: {len(docs)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S3l5u99jS0_",
        "outputId": "972125c8-7c29-4ece-c547-93bae2c15761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Created chunks for each subject:\n",
            "  Subject: big_data, Number of chunks: 224\n",
            "  Subject: pr, Number of chunks: 4582\n",
            "  Subject: ccv, Number of chunks: 599\n"
          ]
        }
      ],
      "source": [
        "#creating chunks with overlap\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def create_chunks(documents):\n",
        "     text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
        "     text_chunks = []\n",
        "     for doc in documents:\n",
        "       # Ensure we are splitting the string content of the document\n",
        "       if isinstance(doc, Document):\n",
        "         text_chunks.extend(text_splitter.split_text(doc.page_content))\n",
        "       elif isinstance(doc, str):\n",
        "         text_chunks.extend(text_splitter.split_text(doc))\n",
        "     return text_chunks\n",
        "\n",
        "for subject,documents in subject_documents.items():\n",
        "  # create chunks for each subject's documents\n",
        "  subject_documents[subject] = create_chunks(documents)\n",
        "\n",
        "# Optional: Print information about the created chunks\n",
        "print(\"\\nCreated chunks for each subject:\")\n",
        "for subject, chunks in subject_documents.items():\n",
        "  print(f\"  Subject: {subject}, Number of chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyI4f79rpMw3"
      },
      "outputs": [],
      "source": [
        "#load the chunks into faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "e_m=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "for subject,chunks in subject_documents.items():\n",
        "  vector_store=FAISS.from_texts(chunks,e_m)\n",
        "  vector_store.save_local(subject)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBSzad8PGlI2"
      },
      "outputs": [],
      "source": [
        "#after retriving connect to the gemenai api key and refine the docs\n",
        "from langchain.chains import RetrievalQA\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Paste your API key here\n",
        "genai.configure(api_key=\"AIzaSyDQS3Eh07DJoI4yBiCPInLBtIfyg8UDiRk\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "id3KiNixzq_y",
        "outputId": "3fe88296-6c18-4010-ecfe-b21a51cd68cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select subject: pr\n",
            "FAISS vector store loaded for subject: pr\n",
            "\n",
            "Ask a question (or type 'exit' to quit): what is pattern recognize\n",
            "The reference material does not explicitly define \"pattern recognition.\" However, it implies that pattern recognition involves:\n",
            "\n",
            "1.  **Achieving a \"good\" representation** of patterns through careful feature selection and extraction. This representation should reveal structural relationships and allow for the expression of the underlying model of the patterns.\n",
            "2.  **Classification:** Identifying the class to which a given pattern belongs, often using training data with known class labels to classify test patterns.\n",
            "\n",
            "Ask a question (or type 'exit' to quit): exit\n",
            "Exiting.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Initialize embedding model (must match the one used during saving)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Set base directory for FAISS indexes\n",
        "data_path = \"/content\"\n",
        "# /content/big_data/index.faiss\n",
        "\n",
        "# Function to load the FAISS index for a given subject\n",
        "def load_faiss_database(subject):\n",
        "    index_dir = os.path.join(data_path, f\"{subject}\")  # no faiss_store_ prefix\n",
        "    if os.path.exists(os.path.join(index_dir, \"index.faiss\")) and os.path.exists(os.path.join(index_dir, \"index.pkl\")):\n",
        "        faiss_db = FAISS.load_local(index_dir, embedding_model, allow_dangerous_deserialization=True)\n",
        "        return faiss_db\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to perform similarity search\n",
        "def query_faiss(query, faiss_db):\n",
        "    similar_docs = faiss_db.similarity_search(query, k=3)\n",
        "    return similar_docs\n",
        "\n",
        "# Ask user to select a subject\n",
        "subject_choice = input(\"Select subject: \").strip().lower()\n",
        "\n",
        "# Load corresponding FAISS database\n",
        "faiss_db = load_faiss_database(subject_choice)\n",
        "\n",
        "if faiss_db:\n",
        "    print(f\"FAISS vector store loaded for subject: {subject_choice}\\n\")\n",
        "    while True:\n",
        "        user_query = input(\"Ask a question (or type 'exit' to quit): \").strip()\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "        similar_docs = query_faiss(user_query, faiss_db)\n",
        "        # Step 3: Combine context\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in similar_docs]) # Extract page_content from Document objects\n",
        "\n",
        "        # Step 4: Ask Gemini to answer using context\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following reference material, answer the user's question.\n",
        "        if it not contain provide your answer\n",
        "\n",
        "        Reference:\n",
        "        {context}\n",
        "\n",
        "        Question: {user_query}\n",
        "        \"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        print(response.text)\n",
        "else:\n",
        "    print(f\"No FAISS index found for subject '{subject_choice}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYWrLnlrcSkN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}